{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./data/cora/\"\n",
    "dataset=\"cora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                    dtype=np.dtype(str))\n",
    "labels = encode_onehot(idx_features_labels[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "edges_transpose = np.transpose(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(shape=(idx.shape[0],idx.shape[0]))\n",
    "for e in edges:\n",
    "    A[e[0]][e[1]]=1\n",
    "X = idx_features_labels[:, 1:-1]\n",
    "Y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=np.eye(idx.shape[0])\n",
    "I=np.eye(idx.shape[0])\n",
    "for e in edges:\n",
    "    D[e[0]][e[0]]=np.sum(edges_transpose[0] == e[0])\n",
    "AI = A + I\n",
    "D_inv = np.linalg.inv(D)\n",
    "D_half = np.dot(D**0.5,D_inv)\n",
    "A_t = np.dot(np.dot(D_half,AI),D_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deg[i] means the no of edges terminating on that vertex ie np.sum(edges_transpose[0]==i)\n",
    "# D is diag(Deg[i]) $\\forall$ i\n",
    "#AI = A + I .. defining that a node i is connected to itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is N X D , Y is N X K , A_t is A X N .. good to go, we need degree matrix Deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 2708) (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print A_t.shape , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = 16 # the dimensions of latent variable\n",
    "H = 32 # the dimension for Hidden unit\n",
    "D = X.shape[1]\n",
    "# W0 would be D X H, W1 would be H X F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = tf.Variable(tf.random_uniform([D, H]),trainable=True)\n",
    "w1 = tf.Variable(tf.random_uniform([H, F]),trainable=True)\n",
    "A_t_tf = tf.convert_to_tensor(A_t, dtype=tf.float32)\n",
    "AI_tf = tf.convert_to_tensor(AI, dtype=tf.float32)\n",
    "X_tf = tf.convert_to_tensor(X,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer :\n",
    "z1 = tf.matmul(tf.matmul(A_t_tf,X_tf),w0) \n",
    "a1 = tf.nn.relu(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Layer :\n",
    "z2 = tf.matmul(tf.matmul(A_t_tf,a1),w1)\n",
    "# a2 = tf.nn.softmax(z2)\n",
    "\n",
    "#Final assignent to Z, it has to be N X F\n",
    "Z = z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the loss function : \n",
    "mat = tf.matmul(Z,tf.transpose(Z))\n",
    "mat_flat=tf.reshape(mat, [-1])\n",
    "A_t_tf_flat = tf.reshape(A_t_tf,[-1])\n",
    "AI_tf_flat = tf.reshape(AI_tf,[-1]) \n",
    "n = X.shape[0]**2 \n",
    "ne = edges.shape[0]\n",
    "ps = (n**2 - ne)/ne\n",
    "norm = n**2/float((n**2-ne)**2)\n",
    "loss = norm*tf.reduce_sum(tf.nn.weighted_cross_entropy_with_logits(targets=AI_tf_flat,logits=mat_flat,pos_weight=ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24762732]\n",
      "[0.22900866]\n",
      "[0.21146204]\n",
      "[0.19498457]\n",
      "[0.17955399]\n",
      "[0.16514073]\n",
      "[0.15171061]\n",
      "[0.13922614]\n",
      "[0.12764716]\n",
      "[0.11693174]\n",
      "[0.10703638]\n",
      "[0.0979169]\n",
      "[0.089528635]\n",
      "[0.081826955]\n",
      "[0.07476768]\n",
      "[0.06830743]\n",
      "[0.062403977]\n",
      "[0.057016447]\n",
      "[0.05210557]\n",
      "[0.04763385]\n",
      "[0.043565746]\n",
      "[0.03986768]\n",
      "[0.036508095]\n",
      "[0.03345752]\n",
      "[0.030688547]\n",
      "[0.0281758]\n",
      "[0.025895804]\n",
      "[0.023826957]\n",
      "[0.021949437]\n",
      "[0.020245122]\n",
      "[0.018697483]\n",
      "[0.017291496]\n",
      "[0.016013503]\n",
      "[0.014851105]\n",
      "[0.013793075]\n",
      "[0.012829293]\n",
      "[0.01195059]\n",
      "[0.0111487135]\n",
      "[0.010416216]\n",
      "[0.009746385]\n",
      "[0.009133206]\n",
      "[0.008571246]\n",
      "[0.008055624]\n",
      "[0.0075819436]\n",
      "[0.0071462668]\n",
      "[0.0067450292]\n",
      "[0.006375037]\n",
      "[0.006033408]\n",
      "[0.005717563]\n",
      "[0.005425167]\n",
      "[0.005154132]\n",
      "[0.0049025514]\n",
      "[0.0046687215]\n",
      "[0.004451103]\n",
      "[0.0042483043]\n",
      "[0.0040590637]\n",
      "[0.0038822412]\n",
      "[0.0037168209]\n",
      "[0.003561861]\n",
      "[0.0034165233]\n",
      "[0.0032800282]\n",
      "[0.0031516752]\n",
      "[0.0030308336]\n",
      "[0.0029169281]\n",
      "[0.0028094356]\n",
      "[0.002707879]\n",
      "[0.0026118206]\n",
      "[0.002520863]\n",
      "[0.0024346479]\n",
      "[0.0023528435]\n",
      "[0.0022751396]\n",
      "[0.002201254]\n",
      "[0.0021309343]\n",
      "[0.0020639456]\n",
      "[0.002000069]\n",
      "[0.0019391032]\n",
      "[0.001880865]\n",
      "[0.0018251904]\n",
      "[0.0017719287]\n",
      "[0.0017209238]\n",
      "[0.0016720428]\n",
      "[0.0016251656]\n",
      "[0.0015801801]\n",
      "[0.0015369791]\n",
      "[0.0014954644]\n",
      "[0.0014555501]\n",
      "[0.0014171442]\n",
      "[0.0013801679]\n",
      "[0.0013445448]\n",
      "[0.0013102078]\n",
      "[0.001277089]\n",
      "[0.0012451323]\n",
      "[0.0012142811]\n",
      "[0.001184482]\n",
      "[0.0011556841]\n",
      "[0.0011278412]\n",
      "[0.0011009123]\n",
      "[0.0010748579]\n",
      "[0.0010496393]\n",
      "[0.0010252201]\n",
      "[0.0010015639]\n",
      "[0.00097864]\n",
      "[0.0009564154]\n",
      "[0.00093486294]\n",
      "[0.00091395626]\n",
      "[0.0008936714]\n",
      "[0.00087398523]\n",
      "[0.0008548746]\n",
      "[0.00083631574]\n",
      "[0.00081828714]\n",
      "[0.0008007688]\n",
      "[0.00078374136]\n",
      "[0.0007671862]\n",
      "[0.00075108965]\n",
      "[0.00073543424]\n",
      "[0.0007202036]\n",
      "[0.0007053831]\n",
      "[0.00069095794]\n",
      "[0.0006769167]\n",
      "[0.0006632427]\n",
      "[0.000649927]\n",
      "[0.00063695665]\n",
      "[0.00062432187]\n",
      "[0.00061201025]\n",
      "[0.00060001126]\n",
      "[0.00058831356]\n",
      "[0.00057690917]\n",
      "[0.0005657883]\n",
      "[0.0005549424]\n",
      "[0.0005443609]\n",
      "[0.000534035]\n",
      "[0.0005239594]\n",
      "[0.00051412673]\n",
      "[0.0005045289]\n",
      "[0.000495159]\n",
      "[0.00048600903]\n",
      "[0.00047707435]\n",
      "[0.00046834844]\n",
      "[0.00045982483]\n",
      "[0.0004514973]\n",
      "[0.00044336138]\n",
      "[0.00043541155]\n",
      "[0.0004276426]\n",
      "[0.00042004883]\n",
      "[0.00041262532]\n",
      "[0.00040536723]\n",
      "[0.0003982698]\n",
      "[0.00039132914]\n",
      "[0.00038454062]\n",
      "[0.0003779013]\n",
      "[0.0003714076]\n",
      "[0.00036505543]\n",
      "[0.00035883998]\n",
      "[0.00035275827]\n",
      "[0.00034680773]\n",
      "[0.0003409844]\n",
      "[0.00033528474]\n",
      "[0.00032970624]\n",
      "[0.0003242448]\n",
      "[0.0003188993]\n",
      "[0.00031366586]\n",
      "[0.00030854176]\n",
      "[0.0003035236]\n",
      "[0.00029860833]\n",
      "[0.00029379374]\n",
      "[0.0002890769]\n",
      "[0.00028445644]\n",
      "[0.0002799288]\n",
      "[0.0002754925]\n",
      "[0.00027114563]\n",
      "[0.00026688588]\n",
      "[0.00026271064]\n",
      "[0.00025861766]\n",
      "[0.00025460534]\n",
      "[0.00025067138]\n",
      "[0.00024681428]\n",
      "[0.00024303136]\n",
      "[0.00023932195]\n",
      "[0.00023568457]\n",
      "[0.00023211722]\n",
      "[0.00022861855]\n",
      "[0.00022518662]\n",
      "[0.0002218198]\n",
      "[0.00021851678]\n",
      "[0.00021527673]\n",
      "[0.00021209777]\n",
      "[0.00020897889]\n",
      "[0.00020591843]\n",
      "[0.00020291506]\n",
      "[0.0001999681]\n",
      "[0.00019707535]\n",
      "[0.00019423493]\n",
      "[0.0001914464]\n",
      "[0.00018870871]\n",
      "[0.0001860208]\n",
      "[0.00018338127]\n",
      "[0.00018078923]\n",
      "[0.000178244]\n",
      "[0.00017574415]\n",
      "[0.000173289]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):\n",
    "        print(sess.run([loss]))\n",
    "        sess.run(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.Variable(10.0, trainable=True)\n",
    "# y = tf.Variable(10.0, trainable=True)\n",
    "\n",
    "\n",
    "# fx = tf.nn.l2_loss(tf.exp(x)+tf.multiply(x,y)-20)\n",
    "# loss = fx\n",
    "# opt = tf.train.GradientDescentOptimizer(0.000002).minimize(fx)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(200):\n",
    "#         print(sess.run([x,y,loss]))\n",
    "#         sess.run(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
